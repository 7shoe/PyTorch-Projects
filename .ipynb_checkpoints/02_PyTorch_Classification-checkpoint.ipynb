{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccda11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import load_dataset, list_datasets, Dataset\n",
    "import huggingface_hub\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f567b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start time \n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db8fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context\n",
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25700141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(data):\n",
    "    data['data'] = data['image'].type(torch.FloatTensor)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cad070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mnist (/eagle/projects/candle_aesp/siebenschuh/HF/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332)\n",
      "Found cached dataset mnist (/eagle/projects/candle_aesp/siebenschuh/HF/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332)\n"
     ]
    }
   ],
   "source": [
    "# load dataset `mnist`\n",
    "mnist_train = load_dataset(path='mnist', \n",
    "                           split='train', \n",
    "                           cache_dir=\"/eagle/projects/candle_aesp/siebenschuh/HF\") #.cast_column(\"image\", float)\n",
    "mnist_test  = load_dataset('mnist', \n",
    "                           split='test', \n",
    "                           cache_dir=\"/eagle/projects/candle_aesp/siebenschuh/HF\") #.cast_column(\"image\", float)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# float tensor\n",
    "dset_train = torchvision.datasets.MNIST(root=\"/eagle/projects/candle_aesp/siebenschuh/PT\", train=True, download=True)\n",
    "dset_test  = torchvision.datasets.MNIST(root=\"/eagle/projects/candle_aesp/siebenschuh/PT\", train=False, download=True)\n",
    "\n",
    "# dsets\n",
    "#mnist_train = mnist_train.with_format(type=\"torch\", device=device)\n",
    "#mnist_test  = mnist_test.with_format(type=\"torch\", device=device)\n",
    "\n",
    "# actual dsets\n",
    "#dset_train = mnist_train.map(transforms, remove_columns=[\"image\"], batched=False)\n",
    "#dset_test = mnist_test.map(transforms, remove_columns=[\"image\"], batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0b2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(device.type!='cuda'):\n",
    "    # Show a digit\n",
    "    digit_id = 5\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(mnist_train['image'][digit_id], cmap='Greys', interpolation='nearest')\n",
    "    plt.title(f'Digit `{mnist_train[\"label\"][digit_id]}`', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2523b",
   "metadata": {},
   "source": [
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b64412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48cada7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81cb4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16 * 4 * 4, 128)\n",
    "        self.fc2   = nn.Linear(128, 64)\n",
    "        self.fc3   = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dim except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7213d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate(batch):\n",
    "    # Convert PIL.Image.Image objects to tensors\n",
    "    batch = [transform(image) for image in batch]\n",
    "    # Return the batch\n",
    "    return torch.stack(batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd825e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "trainloader = torch.utils.data.DataLoader(dset_train, \n",
    "                                          batch_size=b_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1,\n",
    "                                          collate_fn=custom_collate)\n",
    "\n",
    "testloader  = torch.utils.data.DataLoader(dset_test, \n",
    "                                          batch_size=b_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1,\n",
    "                                          collate_fn=custom_collate)\n",
    "\n",
    "classes=tuple([str(i) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0690e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Instance\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc840c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m#inputs, labels = data['image'], data['label']\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data \u001b[38;5;66;03m#['data'].reshape((-1,1,28,28)), data['label']\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[0;32m/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(6):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #inputs, labels = data['image'], data['label']\n",
    "        inputs, labels = data #['data'].reshape((-1,1,28,28)), data['label']\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ffd74d",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(inputs, labels, model):\n",
    "    pred = (model(inputs).argmax(axis=1)).cpu()\n",
    "    gt = labels.cpu()\n",
    "    \n",
    "    # difference\n",
    "    diff = (pred - gt)\n",
    "    diff[diff!=0]=1\n",
    "\n",
    "    return (1. - diff.sum() / len(diff)) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707adc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "accList = []\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    inputs, labels = data['data'].reshape((-1,1,28,28)), data['label']\n",
    "    \n",
    "    accList.append(acc(inputs, labels, net))\n",
    "    \n",
    "print(f'Mean accuracy: {np.mean(accList):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end\n",
    "t1 = time.time()\n",
    "\n",
    "print(f'Comp. time: {t1-t0:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "diff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
